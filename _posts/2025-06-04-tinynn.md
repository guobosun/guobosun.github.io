---
layout: post
title: Build a Deep Learning Framework From Scratch
date: 2019-08-18
update_date: 2019-09-07
categories: DL
description: ä»é›¶å¼€å§‹è®¾è®¡å®ç°ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶
---

<!--START figure-->
<div class="figure">
  <a href="http://ww1.sinaimg.cn/large/006tNc79gy1g645lnfq8uj30j306y0vj.jpg" data-lightbox="tinynn_fead_img">
    <img src="http://ww1.sinaimg.cn/large/006tNc79gy1g645lnfq8uj30j306y0vj.jpg" width="100%" alt="tinynn_fead_img" referrerPolicy="no-referrer"/>
  </a>
</div>
<!--END figure-->

<br>

å½“å‰æ·±åº¦å­¦ä¹ æ¡†æ¶è¶Šæ¥è¶Šæˆç†Ÿï¼Œå¯¹äºä½¿ç”¨è€…è€Œè¨€å°è£…ç¨‹åº¦è¶Šæ¥è¶Šé«˜ï¼Œå¥½å¤„å°±æ˜¯ç°åœ¨å¯ä»¥éå¸¸å¿«é€Ÿåœ°å°†è¿™äº›æ¡†æ¶ä½œä¸ºå·¥å…·ä½¿ç”¨ï¼Œç”¨éå¸¸å°‘çš„ä»£ç å°±å¯ä»¥è¿›è¡Œå®éªŒï¼Œåå¤„å°±æ˜¯å¯èƒ½èƒŒååœ°å®ç°éƒ½è¢«éšè—èµ·æ¥äº†ã€‚åœ¨è¿™ç¯‡æ–‡ç« é‡Œç¬”è€…å°†å¸¦å¤§å®¶ä¸€èµ·ä»å¤´è®¾è®¡å’Œå®ç°ä¸€ä¸ªè½»é‡çº§çš„ï¼ˆå¤§çº¦ 200 è¡Œï¼‰ã€æ˜“äºæ‰©å±•çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ tinynnï¼Œå¸Œæœ›å¯¹å¤§å®¶äº†è§£æ·±åº¦å­¦ä¹ æ¡†æ¶çš„åŸºæœ¬è®¾è®¡å’Œå®ç°æœ‰ä¸€å®šçš„å¸®åŠ©ã€‚

æœ¬æ–‡é¦–å…ˆä¼šä»æ·±åº¦å­¦ä¹ çš„æµç¨‹å¼€å§‹åˆ†æï¼Œå¯¹ç¥ç»ç½‘ç»œä¸­çš„å…³é”®ç»„ä»¶æŠ½è±¡ï¼Œç¡®å®šåŸºæœ¬æ¡†æ¶ï¼›ç„¶åå†å¯¹æ¡†æ¶é‡Œå„ä¸ªç»„ä»¶è¿›è¡Œä»£ç å®ç°ï¼›æœ€ååŸºäºè¿™ä¸ªæ¡†æ¶å®ç°äº†ä¸€ä¸ª MNIST åˆ†ç±»çš„ç¤ºä¾‹ã€‚

<br>

### ç›®å½•

- [ç»„ä»¶æŠ½è±¡](#ç»„ä»¶æŠ½è±¡)
- [ç»„ä»¶å®ç°](#ç»„ä»¶å®ç°)
- [æ•´ä½“ç»“æ„](#æ•´ä½“ç»“æ„)
- [MNIST ä¾‹å­](#mnist-ä¾‹å­)
- [æ€»ç»“](#æ€»ç»“)
- [é™„å½•](#é™„å½•)
- [å‚è€ƒ](#å‚è€ƒ)

---

### ç»„ä»¶æŠ½è±¡

é¦–å…ˆè€ƒè™‘ç¥ç»ç½‘ç»œè¿ç®—çš„æµç¨‹ï¼Œç¥ç»ç½‘ç»œè¿ç®—ä¸»è¦åŒ…å«è®­ç»ƒ training å’Œé¢„æµ‹ predict ï¼ˆæˆ– inferenceï¼‰ ä¸¤ä¸ªé˜¶æ®µï¼Œè®­ç»ƒçš„åŸºæœ¬æµç¨‹æ˜¯ï¼šè¾“å…¥æ•°æ® -> ç½‘ç»œå±‚å‰å‘ä¼ æ’­ -> è®¡ç®—æŸå¤± -> ç½‘ç»œå±‚åå‘ä¼ æ’­æ¢¯åº¦ -> æ›´æ–°å‚æ•°ï¼Œé¢„æµ‹çš„åŸºæœ¬æµç¨‹æ˜¯ è¾“å…¥æ•°æ® -> ç½‘ç»œå±‚å‰å‘ä¼ æ’­ -> è¾“å‡ºç»“æœã€‚ä»è¿ç®—çš„è§’åº¦çœ‹ï¼Œä¸»è¦å¯ä»¥åˆ†ä¸ºä¸‰ç§ç±»å‹çš„è®¡ç®—ï¼š

1. æ•°æ®åœ¨ç½‘ç»œå±‚ç›´æ¥çš„æµåŠ¨

  å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­å¯ä»¥çœ‹åšæ˜¯å¼ é‡ Tensorï¼ˆå¤šç»´æ•°ç»„ï¼‰åœ¨ç½‘ç»œå±‚ä¹‹é—´çš„æµåŠ¨ï¼ˆå‰å‘ä¼ æ’­æµåŠ¨çš„æ˜¯è¾“å…¥è¾“å‡ºï¼Œåå‘ä¼ æ’­æµåŠ¨çš„æ˜¯æ¢¯åº¦ï¼‰ï¼Œæ¯ä¸ªç½‘ç»œå±‚ä¼šè¿›è¡Œä¸€å®šçš„è¿ç®—ï¼Œç„¶åå°†ç»“æœè¾“å…¥ç»™ä¸‹ä¸€å±‚

2. è®¡ç®—æŸå¤±

  è¡”æ¥å‰å‘å’Œåå‘ä¼ æ’­çš„ä¸­é—´è¿‡ç¨‹ï¼Œå®šä¹‰äº†æ¨¡å‹çš„è¾“å‡ºä¸çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ï¼Œç”¨æ¥åç»­æä¾›åå‘ä¼ æ’­æ‰€éœ€çš„ä¿¡æ¯

3. å‚æ•°æ›´æ–°

  ä½¿ç”¨è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦å¯¹ç½‘ç»œå‚æ•°è¿›è¡Œæ›´æ–°çš„ä¸€ç±»è®¡ç®—

åŸºäºè¿™ä¸ªä¸‰ç§ç±»å‹ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹ç½‘ç»œçš„åŸºæœ¬ç»„ä»¶åšä¸€ä¸ªæŠ½è±¡

- `tensor` å¼ é‡ï¼Œè¿™ä¸ªæ˜¯ç¥ç»ç½‘ç»œä¸­æ•°æ®çš„åŸºæœ¬å•ä½
- `layer` ç½‘ç»œå±‚ï¼Œè´Ÿè´£æ¥æ”¶ä¸Šä¸€å±‚çš„è¾“å…¥ï¼Œè¿›è¡Œè¯¥å±‚çš„è¿ç®—ï¼Œå°†ç»“æœè¾“å‡ºç»™ä¸‹ä¸€å±‚ï¼Œç”±äº tensor çš„æµåŠ¨æœ‰å‰å‘å’Œåå‘ä¸¤ä¸ªæ–¹å‘ï¼Œå› æ­¤å¯¹äºæ¯ç§ç±»å‹ç½‘ç»œå±‚æˆ‘ä»¬éƒ½éœ€è¦åŒæ—¶å®ç° forward å’Œ backward ä¸¤ç§è¿ç®—
- `loss` æŸå¤±ï¼Œåœ¨ç»™å®šæ¨¡å‹é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹åï¼Œè¯¥ç»„ä»¶è¾“å‡ºæŸå¤±å€¼ä»¥åŠå…³äºæœ€åä¸€å±‚çš„æ¢¯åº¦ï¼ˆç”¨äºæ¢¯åº¦å›ä¼ ï¼‰
- `optimizer` ä¼˜åŒ–å™¨ï¼Œè´Ÿè´£ä½¿ç”¨æ¢¯åº¦æ›´æ–°æ¨¡å‹çš„å‚æ•°

ç„¶åæˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›ç»„ä»¶æŠŠä¸Šé¢è¿™ä¸ª 4 ç§åŸºæœ¬ç»„ä»¶æ•´åˆåˆ°ä¸€èµ·ï¼Œå½¢æˆä¸€ä¸ª pipeline

- `net` ç»„ä»¶è´Ÿè´£ç®¡ç† tensor åœ¨ layer ä¹‹é—´çš„å‰å‘å’Œåå‘ä¼ æ’­ï¼ŒåŒæ—¶èƒ½æä¾›è·å–å‚æ•°ã€è®¾ç½®å‚æ•°ã€è·å–æ¢¯åº¦çš„æ¥å£
- `model` ç»„ä»¶è´Ÿè´£æ•´åˆæ‰€æœ‰ç»„ä»¶ï¼Œå½¢æˆæ•´ä¸ª pipelineã€‚å³ net ç»„ä»¶è¿›è¡Œå‰å‘ä¼ æ’­ -> loss ç»„ä»¶è®¡ç®—æŸå¤±å’Œæ¢¯åº¦ -> net ç»„ä»¶å°†æ¢¯åº¦åå‘ä¼ æ’­ -> optimizer ç»„ä»¶å°†æ¢¯åº¦æ›´æ–°åˆ°å‚æ•°ã€‚

åŸºæœ¬çš„æ¡†æ¶å›¾å¦‚ä¸‹å›¾

<!--START figure-->
<div class="figure">
  <a href="http://ww4.sinaimg.cn/large/006tNc79gy1g63tkgdh1pj30to0fwjsk.jpg" data-lightbox="tinynn_framework">
    <img src="http://ww4.sinaimg.cn/large/006tNc79gy1g63tkgdh1pj30to0fwjsk.jpg" width="90%" alt="tinynn_framework" referrerPolicy="no-referrer"/>
  </a>
</div>
<!--END figure-->

<br>

### ç»„ä»¶å®ç°

æŒ‰ç…§ä¸Šé¢çš„æŠ½è±¡ï¼Œæˆ‘ä»¬å¯ä»¥å†™å‡ºæ•´ä¸ªæµç¨‹ä»£ç å¦‚ä¸‹ã€‚é¦–å…ˆå®šä¹‰ netï¼Œnet çš„è¾“å…¥æ˜¯å¤šä¸ªç½‘ç»œå±‚ï¼Œç„¶åå°† netã€lossã€optimizer ä¸€èµ·ä¼ ç»™ modelã€‚model å®ç°äº† forwardã€backward å’Œ apply_grad ä¸‰ä¸ªæ¥å£åˆ†åˆ«å¯¹åº”å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°ä¸‰ä¸ªåŠŸèƒ½ã€‚

```python
# define model
net = Net([layer1, layer2, ...])
model = Model(net, loss_fn, optimizer)

# training
pred = model.forward(train_X)
loss, grads = model.backward(pred, train_Y)
model.apply_grad(grads)

# inference
test_pred = model.forward(test_X)
```

æ¥ä¸‹æ¥æˆ‘ä»¬çœ‹è¿™é‡Œè¾¹å„ä¸ªéƒ¨åˆ†åˆ†åˆ«å¦‚ä½•å®ç°ã€‚

- `tensor`

  tensor å¼ é‡æ˜¯ç¥ç»ç½‘ç»œä¸­åŸºæœ¬çš„æ•°æ®å•ä½ï¼Œæˆ‘ä»¬è¿™é‡Œç›´æ¥ä½¿ç”¨ [numpy.ndarray](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html) ç±»ä½œä¸º tensor ç±»çš„å®ç°ï¼ˆnumpy åº•å±‚ä½¿ç”¨äº† C å’Œ Fortranï¼Œå¹¶ä¸”åœ¨ç®—æ³•å±‚é¢è¿›è¡Œäº†å¤§é‡çš„ä¼˜åŒ–ï¼Œè¿ç®—é€Ÿåº¦ä¹Ÿä¸ç®—æ…¢ï¼‰

- `layer `

  ä¸Šé¢æµç¨‹ä»£ç ä¸­ model è¿›è¡Œ forward å’Œ backwardï¼Œå…¶å®åº•å±‚éƒ½æ˜¯ç½‘ç»œå±‚åœ¨è¿›è¡Œå®é™…è¿ç®—ï¼Œå› æ­¤ç½‘ç»œå±‚éœ€è¦æœ‰æä¾› forward å’Œ backward æ¥å£è¿›è¡Œå¯¹åº”çš„è¿ç®—ã€‚åŒæ—¶è¿˜åº”è¯¥å°†è¯¥å±‚çš„å‚æ•°å’Œæ¢¯åº¦è®°å½•ä¸‹æ¥ã€‚å…ˆå®ç°ä¸€ä¸ªåŸºç±»å¦‚ä¸‹

  ```python
  # layer.py
  class Layer(object):
      def __init__(self, name):
          self.name = name
          self.params, self.grads = None, None

      def forward(self, inputs):
          raise NotImplementedError

      def backward(self, grad):
          raise NotImplementedError
  ```

  æœ€åŸºç¡€çš„ä¸€ç§ç½‘ç»œå±‚æ˜¯å…¨è¿æ¥ç½‘ç»œå±‚ï¼Œå®ç°å¦‚ä¸‹ã€‚forward æ–¹æ³•æ¥æ”¶ä¸Šå±‚çš„è¾“å…¥ inputsï¼Œå®ç° $$wx+b$$ çš„è¿ç®—ï¼›backward çš„æ–¹æ³•æ¥æ”¶æ¥è‡ªä¸Šå±‚çš„æ¢¯åº¦ï¼Œè®¡ç®—å…³äºå‚æ•° $$w, b$$ å’Œè¾“å…¥çš„æ¢¯åº¦ï¼Œç„¶åè¿”å›å…³äºè¾“å…¥çš„æ¢¯åº¦ã€‚è¿™ä¸‰ä¸ªæ¢¯åº¦çš„æ¨å¯¼å¯ä»¥è§é™„å½•ï¼Œè¿™é‡Œç›´æ¥ç»™å‡ºå®ç°ã€‚w_init å’Œ b_init åˆ†åˆ«æ˜¯å‚æ•° weight å’Œ bias çš„åˆå§‹åŒ–å™¨ï¼Œè¿™ä¸ªæˆ‘ä»¬åœ¨å¦å¤–çš„ä¸€ä¸ªå®ç°åˆå§‹åŒ–å™¨ä¸­æ–‡ä»¶ `initializer.py` å»å®ç°ï¼Œè¿™éƒ¨åˆ†ä¸æ˜¯æ ¸å¿ƒéƒ¨ä»¶ï¼Œæ‰€ä»¥åœ¨è¿™é‡Œä¸å±•å¼€ä»‹ç»ã€‚

  ```python
  # layer.py
  class Dense(Layer):
      def __init__(self, num_in, num_out,
                   w_init=XavierUniformInit(),
                   b_init=ZerosInit()):
          super().__init__("Linear")

          self.params = {
              "w": w_init([num_in, num_out]),
              "b": b_init([1, num_out])}

          self.inputs = None

      def forward(self, inputs):
          self.inputs = inputs
          return inputs @ self.params["w"] + self.params["b"]

      def backward(self, grad):
          self.grads["w"] = self.inputs.T @ grad
          self.grads["b"] = np.sum(grad, axis=0)
          return grad @ self.params["w"].T
  ```

  åŒæ—¶ç¥ç»ç½‘ç»œä¸­çš„å¦ä¸€ä¸ªé‡è¦çš„éƒ¨åˆ†æ˜¯æ¿€æ´»å‡½æ•°ã€‚æ¿€æ´»å‡½æ•°å¯ä»¥çœ‹åšæ˜¯ä¸€ç§ç½‘ç»œå±‚ï¼ŒåŒæ ·éœ€è¦å®ç° forward å’Œ backward æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡ç»§æ‰¿ Layer ç±»å®ç°æ¿€æ´»å‡½æ•°ç±»ï¼Œè¿™é‡Œå®ç°äº†æœ€å¸¸ç”¨çš„ ReLU æ¿€æ´»å‡½æ•°ã€‚func å’Œ derivation_func æ–¹æ³•åˆ†åˆ«å®ç°å¯¹åº”æ¿€æ´»å‡½æ•°çš„æ­£å‘è®¡ç®—å’Œæ¢¯åº¦è®¡ç®—ã€‚

  ```python
  # layer.py
  class Activation(Layer):
  		"""Base activation layer"""
      def __init__(self, name):
          super().__init__(name)
          self.inputs = None

      def forward(self, inputs):
          self.inputs = inputs
          return self.func(inputs)

      def backward(self, grad):
          return self.derivative_func(self.inputs) * grad

      def func(self, x):
          raise NotImplementedError

      def derivative_func(self, x):
          raise NotImplementedError

  class ReLU(Activation):
  		"""ReLU activation function"""
      def __init__(self):
          super().__init__("ReLU")

      def func(self, x):
          return np.maximum(x, 0.0)

      def derivative_func(self, x):
          return x > 0.0
  ```

- `net`

  ä¸Šæ–‡æåˆ° net ç±»è´Ÿè´£ç®¡ç† tensor åœ¨ layer ä¹‹é—´çš„å‰å‘å’Œåå‘ä¼ æ’­ã€‚forward æ–¹æ³•å¾ˆç®€å•ï¼ŒæŒ‰é¡ºåºéå†æ‰€æœ‰å±‚ï¼Œæ¯å±‚è®¡ç®—çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥ï¼›backward åˆ™é€†åºéå†æ‰€æœ‰å±‚ï¼Œå°†æ¯å±‚çš„æ¢¯åº¦ä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥ã€‚è¿™é‡Œæˆ‘ä»¬è¿˜å°†æ¯ä¸ªç½‘ç»œå±‚å‚æ•°çš„æ¢¯åº¦ä¿å­˜ä¸‹æ¥è¿”å›ï¼Œåé¢å‚æ•°æ›´æ–°éœ€è¦ç”¨åˆ°ã€‚å¦å¤– net ç±»è¿˜å®ç°äº†è·å–å‚æ•°ã€è®¾ç½®å‚æ•°ã€è·å–æ¢¯åº¦çš„æ¥å£ï¼Œä¹Ÿæ˜¯åé¢å‚æ•°æ›´æ–°æ—¶éœ€è¦ç”¨åˆ°

  ```python
  # net.py
  class Net(object):
      def __init__(self, layers):
          self.layers = layers

      def forward(self, inputs):
          for layer in self.layers:
              inputs = layer.forward(inputs)
          return inputs

      def backward(self, grad):
          all_grads = []
          for layer in reversed(self.layers):
              grad = layer.backward(grad)
              all_grads.append(layer.grads)
          return all_grads[::-1]

      def get_params_and_grads(self):
          for layer in self.layers:
              yield layer.params, layer.grads

      def get_parameters(self):
          return [layer.params for layer in self.layers]

      def set_parameters(self, params):
          for i, layer in enumerate(self.layers):
              for key in layer.params.keys():
                  layer.params[key] = params[i][key]
  ```

- `loss`

  ä¸Šæ–‡æˆ‘ä»¬æåˆ° loss ç»„ä»¶éœ€è¦åšä¸¤ä»¶äº‹æƒ…ï¼Œç»™å®šäº†é¢„æµ‹å€¼å’ŒçœŸå®å€¼ï¼Œéœ€è¦è®¡ç®—æŸå¤±å€¼å’Œå…³äºé¢„æµ‹å€¼çš„æ¢¯åº¦ã€‚æˆ‘ä»¬åˆ†åˆ«å®ç°ä¸º loss å’Œ grad ä¸¤ä¸ªæ–¹æ³•ï¼Œè¿™é‡Œæˆ‘ä»¬å®ç°å¤šåˆ†ç±»å›å½’å¸¸ç”¨çš„ SoftmaxCrossEntropyLoss æŸå¤±ã€‚è¿™ä¸ªçš„æŸå¤± loss å’Œæ¢¯åº¦ grad çš„è®¡ç®—å…¬å¼æ¨å¯¼è¿›æ–‡æœ«é™„å½•ï¼Œè¿™é‡Œç›´æ¥ç»™å‡ºç»“æœï¼š

  å¤šåˆ†ç±» softmax äº¤å‰ç†µçš„æŸå¤±ä¸º

  <!--START formula-->
  <div class="formula">
    $$ J_{CE}(y, \hat{y}) = -\sum_{i=1}^N \log \hat{y_i^{c}} $$
  </div>
  <!--END formula-->

  æ¢¯åº¦ç¨å¾®å¤æ‚ä¸€ç‚¹ï¼Œç›®æ ‡ç±»åˆ«å’Œéç›®æ ‡ç±»åˆ«çš„è®¡ç®—å…¬å¼ä¸åŒã€‚å¯¹äºç›®æ ‡ç±»åˆ«ç»´åº¦ï¼Œå…¶æ¢¯åº¦ä¸ºå¯¹åº”ç»´åº¦æ¨¡å‹è¾“å‡ºæ¦‚ç‡å‡ä¸€ï¼Œå¯¹äºéç›®æ ‡ç±»åˆ«ç»´åº¦ï¼Œå…¶æ¢¯åº¦ä¸ºå¯¹åº”ç»´åº¦è¾“å‡ºæ¦‚ç‡æœ¬èº«ã€‚

  <!--START formula-->
  <div class="formula">
    $$ \frac{\partial J_{ce}}{\partial o^c}=
    \begin{cases}
    (\hat{y}^c-1) / N & \text{ç›®æ ‡ç±»åˆ« } c\\
    y^{\tilde{c}} / N & \text{éç›®æ ‡ç±»åˆ« } \tilde{c}
    \end{cases}
    $$
  </div>
  <!--END formula-->

  ä»£ç å®ç°å¦‚ä¸‹

  ```python
  # loss.py
  class BaseLoss(object):
      def loss(self, predicted, actual):
          raise NotImplementedError

      def grad(self, predicted, actual):
          raise NotImplementedError

  class CrossEntropyLoss(BaseLoss):
    	def loss(self, predicted, actual):
          m = predicted.shape[0]
          exps = np.exp(predicted - np.max(predicted, axis=1, keepdims=True))
          p = exps / np.sum(exps, axis=1, keepdims=True)
          nll = -np.log(np.sum(p * actual, axis=1))
          return np.sum(nll) / m

      def grad(self, predicted, actual):
          m = predicted.shape[0]
          grad = np.copy(predicted)
          grad -= actual
          return grad / m
  ```

- `optimizer `

  optimizer ä¸»è¦å®ç°ä¸€ä¸ªæ¥å£ compute_stepï¼Œè¿™ä¸ªæ–¹æ³•æ ¹æ®å½“å‰çš„æ¢¯åº¦ï¼Œè®¡ç®—è¿”å›å®é™…ä¼˜åŒ–æ—¶æ¯ä¸ªå‚æ•°æ”¹å˜çš„æ­¥é•¿ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œå®ç°å¸¸ç”¨çš„ Adam ä¼˜åŒ–å™¨ã€‚

  ```python
  # optimizer.py
  class BaseOptimizer(object):
      def __init__(self, lr, weight_decay):
          self.lr = lr
          self.weight_decay = weight_decay

      def compute_step(self, grads, params):
          step = list()
          # flatten all gradients
          flatten_grads = np.concatenate(
              [np.ravel(v) for grad in grads for v in grad.values()])
          # compute step
          flatten_step = self._compute_step(flatten_grads)
          # reshape gradients
          p = 0
          for param in params:
              layer = dict()
              for k, v in param.items():
                  block = np.prod(v.shape)
                  _step = flatten_step[p:p+block].reshape(v.shape)
                  _step -= self.weight_decay * v
                  layer[k] = _step
                  p += block
              step.append(layer)
          return step

      def _compute_step(self, grad):
          raise NotImplementedError

  class Adam(BaseOptimizer):
      def __init__(self, lr=0.001, beta1=0.9, beta2=0.999,
                   eps=1e-8, weight_decay=0.0):
          super().__init__(lr, weight_decay)
          self._b1, self._b2 = beta1, beta2
          self._eps = eps

          self._t = 0
          self._m, self._v = 0, 0

      def _compute_step(self, grad):
          self._t += 1
          self._m = self._b1 * self._m + (1 - self._b1) * grad
          self._v = self._b2 * self._v + (1 - self._b2) * (grad ** 2)
          # bias correction
          _m = self._m / (1 - self._b1 ** self._t)
          _v = self._v / (1 - self._b2 ** self._t)
          return -self.lr * _m / (_v ** 0.5 + self._eps)

  ```

- `model `

  æœ€å model ç±»å®ç°äº†æˆ‘ä»¬ä¸€å¼€å§‹è®¾è®¡çš„ä¸‰ä¸ªæ¥å£ forwardã€backward å’Œ  apply_grad ï¼Œforward ç›´æ¥è°ƒç”¨ net çš„ forward ï¼Œbackward ä¸­æŠŠ net ã€lossã€optimizer ä¸²èµ·æ¥ï¼Œå…ˆè®¡ç®—æŸå¤± lossï¼Œç„¶ååå‘ä¼ æ’­å¾—åˆ°æ¢¯åº¦ï¼Œç„¶å optimizer è®¡ç®—æ­¥é•¿ï¼Œæœ€åç”± apply_grad å¯¹å‚æ•°è¿›è¡Œæ›´æ–°

  ```python
  # model.py
  class Model(object):
      def __init__(self, net, loss, optimizer):
          self.net = net
          self.loss = loss
          self.optimizer = optimizer

      def forward(self, inputs):
          return self.net.forward(inputs)

      def backward(self, preds, targets):
          loss = self.loss.loss(preds, targets)
          grad = self.loss.grad(preds, targets)
          grads = self.net.backward(grad)
          params = self.net.get_parameters()
          step = self.optimizer.compute_step(grads, params)
          return loss, step

      def apply_grad(self, grads):
          for grad, (param, _) in zip(grads, self.net.get_params_and_grads()):
              for k, v in param.items():
                  param[k] += grad[k]
  ```

<br>

### æ•´ä½“ç»“æ„

æœ€åæˆ‘ä»¬å®ç°å‡ºæ¥æ ¸å¿ƒä»£ç éƒ¨åˆ†æ–‡ä»¶ç»“æ„å¦‚ä¸‹

```
tinynn
â”œâ”€â”€ core
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ initializer.py
â”‚Â Â  â”œâ”€â”€ layer.py
â”‚Â Â  â”œâ”€â”€ loss.py
â”‚Â Â  â”œâ”€â”€ model.py
â”‚Â Â  â”œâ”€â”€ net.py
â”‚Â Â  â””â”€â”€ optimizer.py
```

å…¶ä¸­ `initializer.py` è¿™ä¸ªæ¨¡å—ä¸Šé¢æ²¡æœ‰å±•å¼€è®²ï¼Œä¸»è¦å®ç°äº†å¸¸è§çš„å‚æ•°åˆå§‹åŒ–æ–¹æ³•ï¼Œç”¨äºç»™ç½‘ç»œå±‚åˆå§‹åŒ–å‚æ•°ã€‚

<br>

### MNIST ä¾‹å­

æ¡†æ¶åŸºæœ¬æ­èµ·æ¥åï¼Œæˆ‘ä»¬æ‰¾ä¸€ä¸ªä¾‹å­æ¥ç”¨ tinynn è¿™ä¸ªæ¡†æ¶ run èµ·æ¥ã€‚è¿™ä¸ªä¾‹å­çš„åŸºæœ¬ä¸€äº›é…ç½®å¦‚ä¸‹

- æ•°æ®é›†ï¼š[MNIST](http://yann.lecun.com/exdb/mnist/)
- ä»»åŠ¡ç±»å‹ï¼šå¤šåˆ†ç±»
- ç½‘ç»œç»“æ„ï¼šä¸‰å±‚å…¨è¿æ¥ `INPUT(784) -> FC(400) -> FC(100) -> OUTPUT(10)`ï¼Œè¿™ä¸ªç½‘ç»œæ¥æ”¶ $$(N, 784)$$ çš„è¾“å…¥ï¼Œå…¶ä¸­ $$N$$ æ˜¯æ¯æ¬¡è¾“å…¥çš„æ ·æœ¬æ•°ï¼Œ784 æ˜¯æ¯å¼  $$(28, 28)$$ çš„å›¾åƒå±•å¹³åçš„å‘é‡ï¼Œè¾“å‡ºç»´åº¦ä¸º $$(N, 10)$$ ï¼Œå…¶ä¸­ $$N$$ æ˜¯æ ·æœ¬æ•°ï¼Œ10 æ˜¯å¯¹åº”å›¾ç‰‡åœ¨ 10 ä¸ªç±»åˆ«ä¸Šçš„æ¦‚ç‡
- æ¿€æ´»å‡½æ•°ï¼šReLU
- æŸå¤±å‡½æ•°ï¼šSoftmaxCrossEntropy
- optimizerï¼šAdam(lr=1e-3)
- batch_sizeï¼š128
- Num_epochsï¼š20

è¿™é‡Œæˆ‘ä»¬å¿½ç•¥æ•°æ®è½½å…¥ã€é¢„å¤„ç†ç­‰ä¸€äº›å‡†å¤‡ä»£ç ï¼ŒåªæŠŠæ ¸å¿ƒçš„ç½‘ç»œç»“æ„å®šä¹‰å’Œè®­ç»ƒä»£ç è´´å‡ºæ¥å¦‚ä¸‹

```python
# example/mnist/run.py
net = Net([
  Dense(784, 400),
  ReLU(),
  Dense(400, 100),
  ReLU(),
  Dense(100, 10)
])
model = Model(net=net, loss=SoftmaxCrossEntropyLoss(), optimizer=Adam(lr=args.lr))

iterator = BatchIterator(batch_size=args.batch_size)
evaluator = AccEvaluator()
for epoch in range(num_ep):
    for batch in iterator(train_x, train_y):
      	# training
        pred = model.forward(batch.inputs)
        loss, grads = model.backward(pred, batch.targets)
        model.apply_grad(grads)
    # evaluate every epoch
    test_pred = model.forward(test_x)
    test_pred_idx = np.argmax(test_pred, axis=1)
    test_y_idx = np.asarray(test_y)
    res = evaluator.evaluate(test_pred_idx, test_y_idx)
    print(res)
```

è¿è¡Œç»“æœå¦‚ä¸‹

```
# tinynn
Epoch 0 	 {'total_num': 10000, 'hit_num': 9658, 'accuracy': 0.9658}
Epoch 1 	 {'total_num': 10000, 'hit_num': 9740, 'accuracy': 0.974}
Epoch 2 	 {'total_num': 10000, 'hit_num': 9783, 'accuracy': 0.9783}
Epoch 3 	 {'total_num': 10000, 'hit_num': 9799, 'accuracy': 0.9799}
Epoch 4 	 {'total_num': 10000, 'hit_num': 9805, 'accuracy': 0.9805}
Epoch 5 	 {'total_num': 10000, 'hit_num': 9826, 'accuracy': 0.9826}
Epoch 6 	 {'total_num': 10000, 'hit_num': 9823, 'accuracy': 0.9823}
Epoch 7 	 {'total_num': 10000, 'hit_num': 9819, 'accuracy': 0.9819}
Epoch 8 	 {'total_num': 10000, 'hit_num': 9820, 'accuracy': 0.982}
Epoch 9 	 {'total_num': 10000, 'hit_num': 9838, 'accuracy': 0.9838}
Epoch 10 	 {'total_num': 10000, 'hit_num': 9825, 'accuracy': 0.9825}
Epoch 11 	 {'total_num': 10000, 'hit_num': 9810, 'accuracy': 0.981}
Epoch 12 	 {'total_num': 10000, 'hit_num': 9845, 'accuracy': 0.9845}
Epoch 13 	 {'total_num': 10000, 'hit_num': 9845, 'accuracy': 0.9845}
Epoch 14 	 {'total_num': 10000, 'hit_num': 9835, 'accuracy': 0.9835}
Epoch 15 	 {'total_num': 10000, 'hit_num': 9817, 'accuracy': 0.9817}
Epoch 16 	 {'total_num': 10000, 'hit_num': 9815, 'accuracy': 0.9815}
Epoch 17 	 {'total_num': 10000, 'hit_num': 9835, 'accuracy': 0.9835}
Epoch 18 	 {'total_num': 10000, 'hit_num': 9826, 'accuracy': 0.9826}
Epoch 19 	 {'total_num': 10000, 'hit_num': 9819, 'accuracy': 0.9819}
```

å¯ä»¥çœ‹åˆ°æµ‹è¯•é›† accuracy éšç€è®­ç»ƒè¿›è¡Œåœ¨æ…¢æ…¢æå‡ï¼Œè¿™è¯´æ˜æ•°æ®åœ¨æ¡†æ¶ä¸­ç¡®å®æŒ‰ç…§æ­£ç¡®çš„æ–¹å¼è¿›è¡ŒæµåŠ¨å’Œè®¡ç®—ã€‚ä¸ºäº†å¯¹æ¯”ä¸‹æ•ˆæœï¼Œæˆ‘ç”¨ Tensorflow (1.13.1) å®ç°äº†ç›¸åŒçš„ç½‘ç»œç»“æ„ã€é‡‡ç”¨ç›¸åŒçš„é‡‡æ•°åˆå§‹åŒ–æ–¹æ³•ã€ä¼˜åŒ–å™¨é…ç½®ç­‰ç­‰ï¼Œå¾—åˆ°çš„ç»“æœå¦‚ä¸‹

```
# Tensorflow 1.13.1
Epoch 0 	 {'total_num': 10000, 'hit_num': 9591, 'accuracy': 0.9591}
Epoch 1 	 {'total_num': 10000, 'hit_num': 9734, 'accuracy': 0.9734}
Epoch 2 	 {'total_num': 10000, 'hit_num': 9706, 'accuracy': 0.9706}
Epoch 3 	 {'total_num': 10000, 'hit_num': 9756, 'accuracy': 0.9756}
Epoch 4 	 {'total_num': 10000, 'hit_num': 9722, 'accuracy': 0.9722}
Epoch 5 	 {'total_num': 10000, 'hit_num': 9772, 'accuracy': 0.9772}
Epoch 6 	 {'total_num': 10000, 'hit_num': 9774, 'accuracy': 0.9774}
Epoch 7 	 {'total_num': 10000, 'hit_num': 9789, 'accuracy': 0.9789}
Epoch 8 	 {'total_num': 10000, 'hit_num': 9766, 'accuracy': 0.9766}
Epoch 9 	 {'total_num': 10000, 'hit_num': 9763, 'accuracy': 0.9763}
Epoch 10 	 {'total_num': 10000, 'hit_num': 9791, 'accuracy': 0.9791}
Epoch 11 	 {'total_num': 10000, 'hit_num': 9773, 'accuracy': 0.9773}
Epoch 12 	 {'total_num': 10000, 'hit_num': 9804, 'accuracy': 0.9804}
Epoch 13 	 {'total_num': 10000, 'hit_num': 9782, 'accuracy': 0.9782}
Epoch 14 	 {'total_num': 10000, 'hit_num': 9800, 'accuracy': 0.98}
Epoch 15 	 {'total_num': 10000, 'hit_num': 9837, 'accuracy': 0.9837}
Epoch 16 	 {'total_num': 10000, 'hit_num': 9811, 'accuracy': 0.9811}
Epoch 17 	 {'total_num': 10000, 'hit_num': 9793, 'accuracy': 0.9793}
Epoch 18 	 {'total_num': 10000, 'hit_num': 9818, 'accuracy': 0.9818}
Epoch 19 	 {'total_num': 10000, 'hit_num': 9811, 'accuracy': 0.9811}
```

<!--START figure-->
<div class="figure">
  <a href="http://ww2.sinaimg.cn/large/006tNc79gy1g63yuct3t4j30vc0i0tbg.jpg" data-lightbox="tinynn_vs_tensorflow">
    <img src="http://ww2.sinaimg.cn/large/006tNc79gy1g63yuct3t4j30vc0i0tbg.jpg" width="60%" alt="tinynn_vs_tensorflow" referrerPolicy="no-referrer"/>
  </a>
</div>
<!--END figure-->

å¯ä»¥çœ‹åˆ° ä¸¤è€…æ•ˆæœä¸Šå¤§å·®ä¸å·®ï¼Œå°±å•æ¬¡çš„å®éªŒçœ‹æ¯” Tensorflow ç¨å¾®å¥½ä¸€ç‚¹ç‚¹ã€‚

<br>

### æ€»ç»“

tinynn ç›¸å…³çš„æºä»£ç åœ¨è¿™ä¸ª [repo](https://github.com/borgwang/tinynn) é‡Œã€‚ç›®å‰æ”¯æŒï¼š

- layerï¼šå…¨è¿æ¥å±‚ã€2D å·ç§¯å±‚ã€ 2D åå·ç§¯å±‚ã€MaxPooling å±‚ã€Dropout å±‚ã€BatchNormalization å±‚ã€RNN å±‚ä»¥åŠ ReLUã€Sigmoidã€Tanhã€LeakyReLUã€SoftPlus ç­‰æ¿€æ´»å‡½æ•°
- lossï¼šSigmoidCrossEntropyã€SoftmaxCrossEntroyã€MSEã€MAEã€Huber
- optimizerï¼šRAamã€Adamã€SGDã€RMSPropã€Momentum ç­‰ä¼˜åŒ–å™¨ï¼Œå¹¶ä¸”å¢åŠ äº†åŠ¨æ€è°ƒèŠ‚å­¦ä¹ ç‡ LRScheduler
- å®ç°äº† mnistï¼ˆåˆ†ç±»ï¼‰ã€nn_paintï¼ˆå›å½’ï¼‰ã€DQNï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰ã€AutoEncoder å’Œ DCGAN ï¼ˆæ— ç›‘ç£ï¼‰ç­‰å¸¸è§æ¨¡å‹ã€‚è§ [tinynn/examples](https://github.com/borgwang/tinynn/tree/master/examples)

tinynn è¿˜æœ‰å¾ˆå¤šå¯ä»¥ç»§ç»­å®Œå–„çš„åœ°æ–¹å—é™äºæ—¶é—´è¿˜æ²¡æœ‰å®Œæˆï¼ˆå®ç°å¾ªç¯ç¥ç»ç½‘ç»œå±‚ã€BatchNorm å±‚ã€å¯¹è¿ç®—æ•ˆç‡è¿›è¡Œä¼˜åŒ–ç­‰ç­‰ï¼‰ï¼Œç¬”è€…åœ¨ç©ºé—²æ—¶é—´ä¼šè¿›è¡Œç»´æŠ¤å’Œæ›´æ–°ã€‚

å½“ç„¶ä»ç”Ÿäº§åº”ç”¨çš„è§’åº¦ tinynn å¯èƒ½æ˜¯ä¸€ä¸ªç³Ÿç³•çš„é€‰æ‹©ï¼Œç†ç”±ç”¨ python å®ç°åœ¨è¿™ç§è®¡ç®—å¯†é›†å‹çš„åœºæ™¯ä¸­ä¼šä¸å¯é¿å…åœ°å‡ºç°æ€§èƒ½é—®é¢˜ã€æ²¡æœ‰ GPU æ”¯æŒã€æ²¡æœ‰åˆ†å¸ƒå¼æ”¯æŒã€å¾ˆå¤šç®—æ³•è¿˜æ²¡å®ç°ç­‰ç­‰ç­‰ï¼Œè¿™ä¸ªå°é¡¹ç›®çš„**å‡ºå‘ç‚¹æ›´å¤šåœ°æ˜¯å­¦ä¹ **ï¼Œåœ¨è®¾è®¡å’Œå®ç° tinynn çš„è¿‡ç¨‹ä¸­ç¬”è€…ä¸ªäººå­¦ä¹ ç¡®å®åˆ°äº†å¾ˆå¤šä¸œè¥¿ï¼ŒåŒ…æ‹¬å¦‚ä½•æŠ½è±¡ã€å¦‚ä½•è®¾è®¡ç»„ä»¶æ¥å£ã€å¦‚ä½•æ›´æ•ˆç‡çš„å®ç°ã€ç®—æ³•çš„å…·ä½“ç»†èŠ‚ç­‰ç­‰ã€‚å¯¹ç¬”è€…è€Œè¨€è¿™ä¸ªå†™è¿™ä¸ªå°æ¡†æ¶é™¤äº†äº†è§£æ·±åº¦å­¦ä¹ æ¡†æ¶çš„è®¾è®¡ä¸å®ç°ä¹‹å¤–è¿˜æœ‰ä¸€ä¸ªå¥½å¤„ï¼šåç»­å¯ä»¥**åœ¨è¿™ä¸ªæ¡†æ¶ä¸Šå¿«é€Ÿåœ°å®ç°ä¸€äº›æ–°çš„ç®—æ³•**ï¼Œæ¯”å¦‚æ–°å‡ºäº†æŸç¯‡ paper æå‡ºæ¥æ–°çš„å‚æ•°åˆå§‹åŒ–æ–¹æ³•ï¼Œæ–°çš„ä¼˜åŒ–ç®—æ³•ï¼Œæ–°çš„ç½‘ç»œç»“æ„è®¾è®¡ï¼Œéƒ½å¯ä»¥å¿«é€Ÿåœ°åœ¨è¿™ä¸ªå°æ¡†æ¶ä¸Šå®éªŒã€‚

å¦‚æœä½ å¯¹è‡ªå·±è®¾è®¡å®ç°ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ä¹Ÿæ„Ÿå…´è¶£ï¼Œå¸Œæœ›çœ‹å®Œè¿™ç¯‡æ–‡ç« ä¼šå¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œä¹Ÿæ¬¢è¿å¤§å®¶æ PR ä¸€èµ·è´¡çŒ®ä»£ç ~ ğŸ¤˜

<br>

### é™„å½•

#### Softmax äº¤å‰ç†µæŸå¤±å’Œæ¢¯åº¦æ¨å¯¼

å¤šåˆ†ç±»ä¸‹äº¤å‰ç†µæŸå¤±å¦‚ä¸‹å¼ï¼š

<!--START formula-->
<div class="formula">
  $$ J_{CE}(y, \hat{y}) = -\sum_{i=1}^N\sum_{k=1}^K y_i^k \log \hat{y_i^k} $$
</div>
<!--END formula-->

å…¶ä¸­ $$y, \hat{y}$$ åˆ†åˆ«æ˜¯çœŸå®å€¼å’Œæ¨¡å‹é¢„æµ‹å€¼ï¼Œ$$N$$ æ˜¯æ ·æœ¬æ•°ï¼Œ$$K$$ æ˜¯ç±»åˆ«ä¸ªæ•°ã€‚ç”±äºçœŸå®å€¼ä¸€èˆ¬ä¸ºä¸€ä¸ª one-hot å‘é‡ï¼ˆé™¤äº†çœŸå®ç±»åˆ«ç»´åº¦ä¸º 1 å…¶ä»–å‡ä¸º 0ï¼‰ï¼Œå› æ­¤ä¸Šå¼å¯ä»¥åŒ–ç®€ä¸º

<!--START formula-->
<div class="formula">
  $$ J_{CE}(y, \hat{y}) = -\sum_{i=1}^N \log \hat{y_i^{c}} $$
</div>
<!--END formula-->

å…¶ä¸­ $$c$$ æ˜¯ä»£è¡¨çœŸå®ç±»åˆ«ï¼Œ$$\hat{y_i^c}$$ ä»£è¡¨ç¬¬ $$i$$ ä¸ªæ ·æœ¬ $$c$$ ç±»çš„é¢„æµ‹æ¦‚ç‡ã€‚å³æˆ‘ä»¬éœ€è¦è®¡ç®—çš„æ˜¯æ¯ä¸ªæ ·æœ¬åœ¨çœŸå®ç±»åˆ«ä¸Šçš„é¢„æµ‹æ¦‚ç‡çš„å¯¹æ•°çš„å’Œï¼Œç„¶åå†å–è´Ÿå°±æ˜¯äº¤å‰ç†µæŸå¤±ã€‚

æ¥ä¸‹æ¥æ¨å¯¼å¦‚ä½•æ±‚è§£è¯¥æŸå¤±å…³äºæ¨¡å‹è¾“å‡ºçš„æ¢¯åº¦ï¼Œç”¨ $$o$$ è¡¨ç¤ºæ¨¡å‹è¾“å‡ºï¼Œåœ¨å¤šåˆ†ç±»ä¸­é€šå¸¸æœ€åä¼šä½¿ç”¨ Softmax å°†ç½‘ç»œçš„è¾“å‡ºå½’ä¸€åŒ–ä¸ºä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œåˆ™ Softmax åçš„è¾“å‡ºä¸º

<!--START formula-->
<div class="formula">
  $$ \hat{y}^c = \frac{\exp (o^c)}{\sum_{k=1}^{K} \exp (o^k)} $$
</div>
<!--END formula-->

ä»£å…¥ä¸Šé¢çš„æŸå¤±å‡½æ•°

<!--START formula-->
<div class="formula">
  $$ J_{CE} =-\sum_{i=1}^{N} \left( o_i^c - \log \sum_{k=1}^{K} \exp (o_i^k) \right) $$
</div>
<!--END formula-->

æ±‚è§£ $$J_{CE}$$ å…³äºè¾“å‡ºå‘é‡ $$o$$ çš„æ¢¯åº¦ï¼Œå¯ä»¥å°† $$o$$ åˆ†ä¸ºç›®æ ‡ç±»åˆ«æ‰€åœ¨ç»´åº¦ $$o^c$$ å’Œéç›®æ ‡ç±»åˆ«ç»´åº¦ $$o^{\tilde{c}}$$ã€‚é¦–å…ˆçœ‹ç›®æ ‡ç±»åˆ«æ‰€åœ¨ç»´åº¦ $$o^c$$

<!--START formula-->
<div class="formula">
  $$
  \frac{\partial J_{ce}}{\partial o^c} = -\sum_{i=1}^N \left( 1-\frac{\exp (o^c)}{\sum_{k=1}^{K} \exp (o^k)} \right) = \sum_{i=1}^N(\hat{y}^c-1)
  $$
</div>
<!--END formula-->

å†çœ‹éç›®æ ‡ç±»åˆ«æ‰€åœ¨ç»´åº¦ $$o^{\tilde{c}}$$

<!--START formula-->
<div class="formula">
  $$ \frac{\partial J_{ce}}{\partial o^{\tilde{c}}} = -\sum_{i=1}^N \left( -\frac{\exp (o^c)}{\sum_{k=1}^{K} \exp (o^k)} \right) = \sum_{i=1}^N y^{\tilde{c}} $$
</div>
<!--END formula-->

å¯ä»¥çœ‹åˆ°å¯¹äºç›®æ ‡ç±»åˆ«ç»´åº¦ï¼Œå…¶æ¢¯åº¦ä¸ºå¯¹åº”ç»´åº¦æ¨¡å‹è¾“å‡ºæ¦‚ç‡å‡ä¸€ï¼Œå¯¹äºéç›®æ ‡ç±»åˆ«ç»´åº¦ï¼Œå…¶æ¢¯åº¦ä¸ºå¯¹åº”ç»´åº¦è¾“å‡ºæ¦‚ç‡æœ¬èº«ã€‚

<br>

### å‚è€ƒ

- [Deep Learning, Goodfellow, et al. (2016)](https://www.deeplearningbook.org/)
- [Joel Grus - Livecoding Madness - Let's Build a Deep Learning Library](https://www.youtube.com/watch?v=o64FV-ez6Gw&ab_channel=JoelGrus)
- [TensorFlow Documentation](https://www.tensorflow.org/api_docs)
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)

<br><br>
