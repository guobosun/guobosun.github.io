---
layout: post
title: Automatic Differentiation Tutorial
date: 2019-09-15
categories: DL
description: è‡ªåŠ¨æ±‚å¯¼è®¾è®¡ä¸å®ç°æ€è·¯
---

<!--START figure-->
<div class="figure">
  <a href="https://tva1.sinaimg.cn/large/006y8mN6ly1g70brtzfsaj316a0fq7wh.jpg">
    <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g70brtzfsaj316a0fq7wh.jpg" width="100%" alt="autograd_head_fig" referrerPolicy="no-referrer"/>
  </a>
</div>
<!--END figure-->


### ç›®å½•

- [ç®€ä»‹](#ç®€ä»‹)
- [è‡ªåŠ¨æ±‚å¯¼è®¾è®¡](#è‡ªåŠ¨æ±‚å¯¼è®¾è®¡)
- [è‡ªåŠ¨æ±‚å¯¼å®ç°](#è‡ªåŠ¨æ±‚å¯¼å®ç°)
- [ä¸€ä¸ªä¾‹å­](#ä¸€ä¸ªä¾‹å­)
- [æ€»ç»“](#æ€»ç»“)
- [å‚è€ƒèµ„æ–™](#å‚è€ƒèµ„æ–™)

---

### ç®€ä»‹

æ¢¯åº¦ä¸‹é™ï¼ˆGradient Descentï¼‰åŠå…¶è¡ç”Ÿç®—æ³•æ˜¯ç¥ç»ç½‘ç»œè®­ç»ƒçš„åŸºç¡€ï¼Œæ¢¯åº¦ä¸‹é™æœ¬è´¨ä¸Šå°±æ˜¯æ±‚è§£æŸå¤±å…³äºç½‘ç»œå‚æ•°çš„æ¢¯åº¦ï¼Œä¸æ–­è®¡ç®—è¿™ä¸ªæ¢¯åº¦å¯¹ç½‘ç»œå‚æ•°è¿›è¡Œæ›´æ–°ã€‚ç°ä»£çš„ç¥ç»ç½‘ç»œæ¡†æ¶éƒ½å®ç°äº†è‡ªåŠ¨æ±‚å¯¼çš„åŠŸèƒ½ï¼Œåªéœ€è¦è¦å®šä¹‰å¥½ç½‘ç»œå‰å‘è®¡ç®—çš„é€»è¾‘ï¼Œåœ¨è¿ç®—æ—¶è‡ªåŠ¨æ±‚å¯¼æ¨¡å—å°±ä¼šè‡ªåŠ¨æŠŠæ¢¯åº¦ç®—å¥½ï¼Œä¸ç”¨è‡ªå·±æ‰‹å†™æ±‚å¯¼æ¢¯åº¦ã€‚

ç¬”è€…åœ¨ä¹‹å‰çš„ [ä¸€ç¯‡æ–‡ç« ](<https://borgwang.github.io/dl/2019/08/18/tinynn.html>) ä¸­è®²è§£å’Œå®ç°äº†ä¸€ä¸ªè¿·ä½ çš„ç¥ç»ç½‘ç»œæ¡†æ¶ tinynnï¼Œåœ¨ tinynn ä¸­æˆ‘ä»¬å®šä¹‰äº†ç½‘ç»œå±‚ layer çš„æ¦‚å¿µï¼Œæ•´ä¸ªç½‘ç»œæ˜¯ç”±ä¸€å±‚å±‚çš„ layer å èµ·æ¥çš„ï¼ˆå…¨è¿æ¥å±‚ã€å·ç§¯å±‚ã€æ¿€æ´»å‡½æ•°å±‚ã€Pooling å±‚ç­‰ç­‰ï¼‰ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º

<!--START figure-->
<div class="figure">
  <a href="http://ww4.sinaimg.cn/large/006tNc79gy1g63tkgdh1pj30to0fwjsk.jpg" data-lightbox="tinynn_framework">
    <img src="http://ww4.sinaimg.cn/large/006tNc79gy1g63tkgdh1pj30to0fwjsk.jpg" width="90%" alt="tinynn_framework" referrerPolicy="no-referrer"/>
  </a>
</div>
<!--END figure-->

åœ¨å®ç°çš„æ—¶å€™éœ€è¦æ˜¾ç¤ºä¸ºæ¯å±‚å®šä¹‰å¥½å‰å‘ forward å’Œåå‘ backwardï¼ˆæ¢¯åº¦è®¡ç®—ï¼‰çš„è®¡ç®—é€»è¾‘ã€‚ä»æœ¬è´¨ä¸Šçœ‹ è¿™äº› layer å…¶å®æ˜¯ä¸€ç»„åŸºç¡€ç®—å­çš„ç»„åˆï¼Œè€Œè¿™äº›åŸºç¡€ç®—å­ï¼ˆåŠ å‡ä¹˜é™¤ã€çŸ©é˜µå˜æ¢ç­‰ç­‰ï¼‰çš„å¯¼å‡½æ•°æœ¬èº«éƒ½æ¯”è¾ƒç®€å•ï¼Œå¦‚æœèƒ½å¤Ÿ**å°†è¿™äº›åŸºç¡€ç®—å­çš„å¯¼å‡½æ•°å†™å¥½ï¼ŒåŒæ—¶æŠŠä¸åŒç®—å­ä¹‹é—´è¿æ¥é€»è¾‘è®°å½•ï¼ˆè®¡ç®—ä¾èµ–å›¾ï¼‰ä¸‹æ¥ï¼Œé‚£ä¹ˆè¿™ä¸ªæ—¶å€™å°±ä¸å†éœ€è¦è‡ªå·±å†™åå‘äº†ï¼Œåªéœ€è¦è®¡ç®—æŸå¤±ï¼Œç„¶åä»æŸå¤±å‡½æ•°å¼€å§‹ï¼Œè®©æ¢¯åº¦è‡ªå·±ç”¨é¢„å…ˆå®šä¹‰å¥½çš„å¯¼å‡½æ•°ï¼Œæ²¿ç€è®¡ç®—å›¾åå‘æµåŠ¨å³å¯ä»¥å¾—åˆ°å‚æ•°çš„æ¢¯åº¦**ï¼Œè¿™ä¸ªå°±æ˜¯è‡ªåŠ¨æ±‚å¯¼çš„æ ¸å¿ƒæ€æƒ³ã€‚tinynn ä¸­ä¹‹æ‰€æœ‰ layer è¿™ä¸ªæ¦‚å¿µï¼Œä¸€æ–¹é¢æ˜¯ç¬¦åˆæˆ‘ä»¬ç›´è§‰ä¸Šçš„ç†è§£ï¼Œå¦ä¸€æ–¹é¢æ˜¯ä¸ºäº†åœ¨æ²¡æœ‰è‡ªåŠ¨æ±‚å¯¼çš„æƒ…å†µä¸‹æ–¹ä¾¿å®ç°ã€‚æœ‰äº†è‡ªåŠ¨æ±‚å¯¼ï¼Œæˆ‘ä»¬å¯ä»¥æŠ›å¼€ layer è¿™ä¸ªæ¦‚å¿µï¼Œç¥ç»ç½‘ç»œçš„è®­ç»ƒå¯ä»¥æŠ½è±¡ä¸ºå®šä¹‰å¥½ä¸€ä¸ªç½‘ç»œçš„è®¡ç®—å›¾ï¼Œç„¶åè®©æ•°æ®å‰å‘æµåŠ¨ï¼Œè®©æ¢¯åº¦è‡ªåŠ¨åå‘æµåŠ¨ï¼ˆ TensorFlow è¿™ä¸ªåå­—èµ·å¾—ç›¸å½“æœ‰æ°´å‡†ï¼‰ã€‚

æˆ‘ä»¬å¯ä»¥çœ‹çœ‹ PyTorch çš„ä¸€å°æ®µæ ¸å¿ƒçš„è®­ç»ƒä»£ç ï¼ˆæ¥æº[å®˜æ–¹æ–‡æ¡£ MNIST ä¾‹å­](https://github.com/pytorch/examples/blob/master/mnist/main.py)ï¼‰

```python
for batch_idx, (data, target) in enumerate(train_loader):
    data, target = data.to(device), target.to(device)
    optimizer.zero_grad()  # åˆå§‹åŒ–æ¢¯åº¦
    output = model(data)  # ä» data åˆ° output çš„è®¡ç®—å›¾
    loss = F.nll_loss(output, target) # ä» output åˆ° loss çš„è®¡ç®—å›¾
    loss.backward()  # æ¢¯åº¦ä» loss å¼€å§‹åå‘æµåŠ¨
    optimizer.step()  # ä½¿ç”¨æ¢¯åº¦å¯¹å‚æ•°æ›´æ–°
```

å¯ä»¥çœ‹åˆ° PyTorch çš„åŸºæœ¬æ€è·¯å’Œæˆ‘ä»¬ä¸Šé¢æè¿°çš„æ˜¯ä¸€è‡´çš„ï¼Œå®šä¹‰å¥½è®¡ç®—å›¾ -> forward å¾—åˆ°æŸå¤± -> æ¢¯åº¦åå‘æµåŠ¨ã€‚

<br>

### è‡ªåŠ¨æ±‚å¯¼è®¾è®¡

çŸ¥é“äº†è‡ªåŠ¨æ±‚å¯¼çš„åŸºæœ¬æµç¨‹ä¹‹åï¼Œæˆ‘ä»¬è€ƒè™‘å¦‚ä½•æ¥å®ç°ã€‚å…ˆè€ƒè™‘æ²¡æœ‰è‡ªåŠ¨æ±‚å¯¼ï¼Œä¸ºæ¯ä¸ªè¿ç®—æ‰‹åŠ¨å†™ backward çš„æƒ…å†µï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬å®é™…ä¸Šå®šä¹‰äº†ä¸¤ä¸ªè®¡ç®—å›¾ï¼Œä¸€ä¸ªå‰å‘ä¸€ä¸ªåå‘ï¼Œè€ƒè™‘æœ€ç®€å•çš„çº¿æ€§å›å½’çš„è¿ç®— $$WX + B$$ï¼Œå…¶è®¡å¦‚ä¸‹æ‰€ç¤ºã€‚

<!--START figure-->
<div class="figure">
  <a href="https://tva1.sinaimg.cn/large/006y8mN6ly1g6zp3sw80gj312s0sq0yy.jpg">
    <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6zp3sw80gj312s0sq0yy.jpg" width="80%" alt="compute_graph" referrerPolicy="no-referrer"/>
  </a>
</div>
<!--END figure-->

å¯ä»¥çœ‹åˆ°è¿™ä¸¤ä¸ªè®¡ç®—å›¾çš„ç»“æ„å®é™…ä¸Šæ˜¯ä¸€æ ·çš„ï¼Œåªæ˜¯åœ¨å‰å‘æµåŠ¨çš„æ˜¯è®¡ç®—çš„ä¸­é—´ç»“æœï¼Œåå‘æµåŠ¨çš„æ˜¯æ¢¯åº¦ï¼Œä»¥åŠä¸­é—´çš„è¿ç®—åå‘çš„æ—¶å€™æ˜¯å¯¼æ•°è¿ç®—ã€‚å®é™…ä¸Šæˆ‘ä»¬å¯ä»¥æŠŠä¸¤è€…ç»“åˆåˆ°ä¸€èµ·ï¼Œåªå®šä¹‰ä¸€æ¬¡å‰å‘è®¡ç®—å›¾ï¼Œè®©åå‘è®¡ç®—å›¾è‡ªåŠ¨ç”Ÿæˆ

<!--START figure-->
<div class="figure">
  <a href="https://tva1.sinaimg.cn/large/006y8mN6ly1g706spz5j3j30my0s6n13.jpg">
    <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g706spz5j3j30my0s6n13.jpg" width="50%" alt="compute_graph2" referrerPolicy="no-referrer"/>
  </a>
</div>
<!--END figure-->

ä»å®ç°çš„è§’åº¦çœ‹ï¼Œå¦‚æœæˆ‘ä»¬ä¸éœ€è¦è‡ªåŠ¨æ±‚å¯¼ï¼Œé‚£ä¹ˆç½‘ç»œæ¡†æ¶ä¸­çš„ Tensor ç±»åªéœ€è¦å¯¹ Tensor è¿ç®—ç¬¦æœ‰å®šä¹‰ï¼Œèƒ½å¤Ÿè¿›è¡Œæ•°å€¼è¿ç®—ï¼ˆtinynn ä¸­å°±ç®€å•çš„ä½¿ç”¨ ndarray ä½œä¸º Tensor çš„å®ç°ï¼‰ã€‚ä½†å¦‚æœè¦å®ç°è‡ªåŠ¨æ±‚å¯¼ï¼Œé‚£ä¹ˆ Tensor ç±»éœ€è¦é¢å¤–åšå‡ ä»¶äº‹ï¼š

1. å¢åŠ ä¸€ä¸ªæ¢¯åº¦çš„å˜é‡ä¿å­˜å½“å‰ tensor çš„æ¢¯åº¦
2. ä¿å­˜å½“å‰ tensor ä¾èµ–çš„ tensorï¼ˆå¦‚ä¸Šå›¾ä¸­ $$O_1$$ ä¾èµ–äº $$X, W$$ï¼‰
3. ä¿å­˜ä¸‹å¯¹å„ä¸ªä¾èµ– tensor çš„å¯¼å‡½æ•°ï¼ˆè¿™ä¸ªå¯¼å‡½æ•°çš„ä½œç”¨æ˜¯å°†å½“å‰ tensor çš„æ¢¯åº¦ä¼ åˆ°ä¾èµ–çš„ tensor ä¸Šï¼‰

<br>

### è‡ªåŠ¨æ±‚å¯¼å®ç°

æˆ‘ä»¬æŒ‰ç…§ä¸Šé¢çš„åˆ†æå¼€å§‹å®ç° Tensor ç±»å¦‚ä¸‹ï¼Œåˆå§‹åŒ–æ–¹æ³•ä¸­é¦–å…ˆæŠŠ tensor çš„å€¼ä¿å­˜ä¸‹æ¥ï¼Œç„¶åæœ‰ä¸€ä¸ª `requires_grad` çš„ bool å˜é‡è¡¨æ˜è¿™ä¸ª tensor æ˜¯ä¸æ˜¯éœ€è¦æ±‚æ¢¯åº¦ï¼Œè¿˜æœ‰ä¸€ä¸ª dependency çš„åˆ—è¡¨ç”¨äºä¿å­˜è¯¥ tensor ä¾èµ–çš„ tensor ä»¥åŠå¯¹äºä»–ä»¬çš„å¯¼å‡½æ•°ã€‚

`zero_grad()` æ–¹æ³•æ¯”è¾ƒç®€å•ï¼Œå°†å½“å‰ tensor çš„æ¢¯åº¦è®¾ç½®ä¸º 0ï¼Œé˜²æ­¢æ¢¯åº¦çš„ç´¯åŠ ã€‚è‡ªåŠ¨æ±‚å¯¼ä»è°ƒç”¨è®¡ç®—å›¾çš„æœ€åä¸€ä¸ªèŠ‚ç‚¹ tensor çš„ `backward()` æ–¹æ³•å¼€å§‹ï¼ˆåœ¨ç¥ç»ç½‘ç»œä¸­è¿™ä¸ªèŠ‚ç‚¹ä¸€èˆ¬æ˜¯ lossï¼‰ã€‚`backward()` æ–¹æ³•ä¸»è¦æµç¨‹ä¸º

- ç¡®ä¿æ”¹ tensor ç¡®å®éœ€è¦æ±‚å¯¼ `self.requires_grad == True`
- å°†ä»ä¸Šä¸ª tensor ä¼ è¿›æ¥çš„æ¢¯åº¦åŠ åˆ°è‡ªèº«æ¢¯åº¦ä¸Šï¼Œå¦‚æœæ²¡æœ‰ï¼ˆåå‘æ±‚å¯¼çš„èµ·ç‚¹ tensorï¼‰ï¼Œåˆ™å°†æ¢¯åº¦åˆå§‹åŒ–ä¸º 1.0
- **å¯¹æ¯ä¸€ä¸ªä¾èµ–çš„ tensor è¿è¡Œä¿å­˜ä¸‹æ¥çš„å¯¼å‡½æ•°ï¼Œè®¡ç®—ä¼ æ’­åˆ°ä¾èµ– tensor çš„æ¢¯åº¦ï¼Œç„¶åè°ƒç”¨ä¾èµ– tensor çš„ `backward()` æ–¹æ³•**ã€‚å¯ä»¥çœ‹åˆ°è¿™å…¶å®å°±æ˜¯ Depth-First Search è®¡ç®—å›¾çš„èŠ‚ç‚¹

```python
def as_tensor(obj):
    if not isinstance(obj, Tensor):
        obj = Tensor(obj)
    return obj


class Tensor:

    def __init__(self, values, requires_grad=False, dependency=None):
        self._values = np.array(values)
        self.shape = self.values.shape

        self.grad = None
        if requires_grad:
            self.zero_grad()
        self.requires_grad = requires_grad

        if dependency is None:
            dependency = []
        self.dependency = dependency

    @property
    def values(self):
        return self._values

    @values.setter
    def values(self, new_values):
        self._values = np.array(new_values)
        self.grad = None

    def zero_grad(self):
        self.grad = np.zeros(self.shape)

    def backward(self, grad=None):
        assert self.requires_grad, "Call backward() on a non-requires-grad tensor."
        grad = 1.0 if grad is None else grad
        grad = np.array(grad)

        # accumulate gradient
        self.grad += grad

        # propagate the gradient to its dependencies
        for dep in self.dependency:
            grad_for_dep = dep["grad_fn"](grad)
            dep["tensor"].backward(grad_for_dep)
```

å¯èƒ½çœ‹åˆ°è¿™é‡Œè¯»è€…å¯èƒ½ä¼šç–‘é—®ï¼Œä¸€ä¸ª tensor ä¾èµ–çš„ tensor å’Œå¯¹ä»–ä»¬çš„å¯¼å‡½æ•°ï¼ˆä¹Ÿå°±æ˜¯ `dependency` é‡Œé¢çš„ä¸œè¥¿ï¼‰ä»å“ªé‡Œæ¥ï¼Ÿä¼¼ä¹æ²¡æœ‰å“ªä¸€ä¸ªæ–¹æ³•åœ¨åšä¿å­˜ä¾èµ–è¿™ä»¶äº‹ã€‚

å‡è®¾æˆ‘ä»¬å¯èƒ½ä¼šè¿™æ ·ä½¿ç”¨æˆ‘ä»¬çš„ Tensor ç±»

```python
W = Tensor([[1], [3]], requires_grad=True)  # 2x1 tensor
X = Tensor([[1, 2], [3, 4], [5, 6], [7, 8]], requires_grad=True)  # 4x2 tensor
O = X @ W  # suppose to be a 4x1 tensor
```

å¦‚ä½•è®© `X` å’Œ `W` å®ŒæˆçŸ©é˜µä¹˜æ³•è¾“å‡ºæ­£ç¡®çš„ `O` çš„åŒæ—¶ï¼Œè®© `O` èƒ½è®°ä¸‹ä»–ä¾èµ–äº `W` å’Œ `X` å‘¢ï¼Ÿç­”æ¡ˆæ˜¯**é‡è½½è¿ç®—ç¬¦**ã€‚

```python
class Tensor:
    # ...
    def __matmul__(self, other):
        # 1. calculate forward values
        values = self.values @ other.values

        # 2. if output tensor requires_grad
        requires_grad = ts1.requires_grad or ts2.requires_grad

        # 3. build dependency list
        dependency = []
        if self.requires_grad:
            # O = X @ W
            # D_O / D_X = grad @ W.T
            def grad_fn1(grad):
            	return grad @ other.values.T
            dependency.append(dict(tensor=self, grad_fn=grad_fn1))
        if other.requires_grad:
            # O = X @ W
            # D_O / D_W = X.T @ grad
			def grad_fn2(grad):
            	return self.values.T @ grad
            dependency.append(dict(tensor=other, grad_fn=grad_fn2))
        return Tensor(values, requires_grad, dependency)
    # ...
```

å…³äº Python ä¸­å¦‚ä½•é‡è½½è¿ç®—ç¬¦è¿™é‡Œä¸å±•å¼€ï¼Œè¯»è€…æœ‰å…´è¶£å¯ä»¥å‚è€ƒå®˜æ–¹æ–‡æ¡£æˆ–è€…[è¿™ç¯‡æ–‡ç« ](https://rszalski.github.io/magicmethods/)ã€‚åŸºæœ¬ä¸Šåœ¨ Tensor ç±»å†…å®šä¹‰äº† `__matmul__` è¿™ä¸ªæ–¹æ³•åï¼Œå®é™…ä¸Šæ˜¯é‡è½½äº†çŸ©é˜µä¹˜æ³•è¿ç®—ç¬¦ `@` ï¼ˆPython 3.5 ä»¥ä¸Šæ”¯æŒï¼‰ ã€‚å½“è¿è¡Œ `X @ W` æ—¶ä¼šè‡ªåŠ¨è°ƒç”¨ `X` çš„ `__matmul__` æ–¹æ³•ã€‚

è¿™ä¸ªæ–¹æ³•é‡Œé¢åšäº†ä¸‰ä»¶äº‹ï¼š

1. è®¡ç®—çŸ©é˜µä¹˜æ³•ç»“æœï¼ˆè¿™ä¸ªæ˜¯å¿…é¡»çš„ï¼‰

2. ç¡®å®šæ˜¯å¦éœ€è¦æ–°ç”Ÿæˆçš„ tensor æ˜¯å¦éœ€è¦æ¢¯åº¦ï¼Œè¿™ä¸ªç”±ä¸¤ä¸ªæ“ä½œæ•°å†³å®šã€‚æ¯”å¦‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå¦‚æœ `W` æˆ–è€… `X` éœ€è¦æ¢¯åº¦ï¼Œé‚£ä¹ˆç”Ÿæˆçš„ `O`ä¹Ÿæ˜¯éœ€è¦è®¡ç®—æ¢¯åº¦çš„ï¼ˆè¿™æ ·æ‰èƒ½å¤Ÿè®¡ç®— `W` æˆ–è€… `X` çš„æ¢¯åº¦ï¼‰

3. å»ºç«‹ tensor çš„ä¾èµ–åˆ—è¡¨

   è‡ªåŠ¨æ±‚å¯¼ä¸­æœ€å…³é”®çš„éƒ¨åˆ†å°±æ˜¯åœ¨è¿™é‡Œäº†ï¼Œè¿˜æ˜¯ä»¥ `O = X @ W` ä¸ºä¾‹å­ï¼Œè¿™é‡Œæˆ‘ä»¬ä¼šå…ˆæ£€æŸ¥æ˜¯å¦ `X`éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œå¦‚æœéœ€è¦ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå¯¼å‡½æ•° `D_O / D_X` å®šä¹‰å¥½ï¼Œä¿å­˜ä¸‹æ¥ï¼›åŒæ ·çš„å¦‚æœ `W` éœ€è¦æ¢¯åº¦ï¼Œæˆ‘ä»¬å°† `D_O / D_W` å®šä¹‰å¥½ä¿å­˜ä¸‹æ¥ã€‚æœ€åç”Ÿæˆä¸€ä¸ª dependency åˆ—è¡¨ä¿å­˜ç€åœ¨æ–°ç”Ÿæˆçš„ tensor `O` ä¸­ã€‚

ç„¶åæˆ‘ä»¬å†å›é¡¾å‰é¢è®²çš„ `backward()`æ–¹æ³•ï¼Œ`backward()` æ–¹æ³•ä¼šéå† tensor çš„ dependency ï¼Œå°†ç”¨ä¿å­˜çš„ grad_fn è®¡ç®—è¦ä¼ ç»™ä¾èµ– tensor çš„æ¢¯åº¦ï¼Œç„¶åè°ƒç”¨ä¾èµ– tensor çš„ `backward()` æ–¹æ³•å°†æ¢¯åº¦ä¼ é€’ä¸‹å»ï¼Œä»è€Œå®ç°äº†æ¢¯åº¦åœ¨æ•´ä¸ªè®¡ç®—å›¾çš„æµåŠ¨ã€‚

```python
grad_for_dep = dep["grad_fn"](grad)
dep["tensor"].backward(grad_for_dep)
```

è‡ªåŠ¨æ±‚å¯¼è®²åˆ°è¿™é‡Œå…¶å®å·²ç»åŸºæœ¬æ²¡æœ‰ä»€ä¹ˆæ–°ä¸œè¥¿ï¼Œå‰©ä¸‹çš„å·¥ä½œå°±æ˜¯ä»¥ç±»ä¼¼çš„æ–¹æ³•å¤§é‡åœ°é‡è½½å„ç§å„æ ·çš„è¿ç®—ç¬¦ï¼Œä½¿å…¶èƒ½å¤Ÿ cover ä½å¤§éƒ¨åˆ†æ‰€éœ€è¦çš„æ“ä½œï¼ˆåŸºæœ¬ä¸Šç…§ç€ NumPy çš„æ¥å£éƒ½ç»™é‡è½½ä¸€æ¬¡å°±å·®ä¸å¤šäº† ğŸ¤¨ï¼‰ã€‚æ— è®ºä½ å®šä¹‰äº†å¤šå¤æ‚çš„è¿ç®—ï¼Œåªè¦é‡è½½äº†ç›¸å…³çš„è¿ç®—ç¬¦ï¼Œå°±éƒ½èƒ½å¤Ÿè‡ªåŠ¨æ±‚å¯¼äº†ï¼Œå†ä¹Ÿä¸ç”¨è‡ªå·±å†™æ¢¯åº¦äº†ã€‚

<!--START figure-->
<div class="figure">
  <a href="https://tva1.sinaimg.cn/large/006y8mN6ly1g70bkhamymj307708mgp6.jpg">
    <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g70bkhamymj307708mgp6.jpg" width="40%" alt="autograd_meme" referrerPolicy="no-referrer"/>
  </a>
</div>
<!--END figure-->

<br>

### ä¸€ä¸ªä¾‹å­

å¤§é‡çš„é‡è½½è¿ç®—ç¬¦çš„å·¥ä½œåœ¨æ–‡ç« é‡Œå°±ä¸è´´ä¸Šæ¥äº†ï¼ˆè¿‡ç¨‹ä¸æ€ä¹ˆæœ‰è¶£ï¼‰ï¼Œæˆ‘å†™åœ¨äº†ä¸€ä¸ª notebook ä¸Šï¼Œå¤§å®¶æœ‰å…´è¶£å¯ä»¥å»çœ‹çœ‹ [borgwang/toys/ml-autograd](https://github.com/borgwang/toys/tree/master/ml-autograd)ã€‚åœ¨è¿™ä¸ª notebook é‡Œé¢é‡è½½äº†å®ç°ä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’éœ€è¦çš„å‡ ç§è¿ç®—ç¬¦ï¼Œä»¥åŠä¸€ä¸ªçº¿æ€§å›å½’çš„ä¾‹å­ã€‚è¿™é‡ŒæŠŠä¾‹å­å’Œç»“æœè´´ä¸Šæ¥

```python
# training data
x = Tensor(np.random.normal(0, 1.0, (100, 3)))
coef = Tensor(np.random.randint(0, 10, (3,)))
y = x * coef - 3

params = {
    "w": Tensor(np.random.normal(0, 1.0, (3, 3)), requires_grad=True),
    "b": Tensor(np.random.normal(0, 1.0, 3), requires_grad=True)
}

learng_rate = 3e-4
loss_list = []
for e in range(101):
    # set gradient to zero
    for param in params.values():
        param.zero_grad()

    # forward
    predicted = x @ params["w"] + params["b"]
    err = predicted - y
    loss = (err * err).sum()

    # backward automatically
    loss.backward()

    # updata parameters (gradient descent)
    for param in params.values():
        param -= learng_rate * param.grad

    loss_list.append(loss.values)
    if e % 10 == 0:
        print("epoch-%i \tloss: %.4f" % (e, loss.values))
```

```
epoch-0 	loss: 8976.9821
epoch-10 	loss: 2747.4262
epoch-20 	loss: 871.4415
epoch-30 	loss: 284.9750
epoch-40 	loss: 95.7080
epoch-50 	loss: 32.9175
epoch-60 	loss: 11.5687
epoch-70 	loss: 4.1467
epoch-80 	loss: 1.5132
epoch-90 	loss: 0.5611
epoch-100 	loss: 0.2111
```

<!--START figure-->
<div class="figure">
  <a href="https://tva1.sinaimg.cn/large/006y8mN6ly1g70a9h3g2kj30wm0j8abh.jpg">
    <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g70a9h3g2kj30wm0j8abh.jpg" width="60%" alt="autograd_lr_result" referrerPolicy="no-referrer"/>
  </a>
</div>
<!--END figure-->

æ¥å£å’Œ PyTorch ç›¸ä¼¼ï¼Œåœ¨æ¯ä¸ªå¾ªç¯é‡Œé¢é¦–å…ˆå°†å‚æ•°æ¢¯åº¦è®¾ä¸º 0 ï¼Œç„¶åå®šä¹‰è®¡ç®—å›¾ï¼Œç„¶åä» loss å¼€å§‹åå‘ä¼ æ’­ï¼Œæœ€åæ›´æ–°å‚æ•°ã€‚ä»ç»“æœå¯ä»¥çœ‹åˆ° loss éšç€è®­ç»ƒè¿›è¡Œéå¸¸æ¼‚äº®åœ°ä¸‹é™ï¼Œè¯´æ˜æˆ‘ä»¬çš„è‡ªåŠ¨æ±‚å¯¼æŒ‰ç…§æˆ‘ä»¬çš„è®¾æƒ³ work äº†ã€‚

<br>

### æ€»ç»“

æœ¬æ–‡å®ç°äº†è®¨è®ºäº†è‡ªåŠ¨æ±‚å¯¼çš„è®¾è®¡æ€è·¯å’Œæ•´ä¸ªè¿‡ç¨‹æ˜¯æ€ä¹ˆè¿ä½œçš„ã€‚æ€»ç»“èµ·æ¥ï¼šè‡ªåŠ¨æ±‚å¯¼å°±æ˜¯åœ¨å®šä¹‰äº†ä¸€ä¸ªæœ‰çŠ¶æ€çš„è®¡ç®—å›¾ï¼Œè¯¥è®¡ç®—å›¾ä¸Šçš„èŠ‚ç‚¹ä¸ä»…ä¿å­˜äº†èŠ‚ç‚¹çš„å‰å‘è¿ç®—ï¼Œè¿˜ä¿å­˜äº†åå‘è®¡ç®—æ‰€éœ€çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé€šè¿‡å›¾éå†è®©æ¢¯åº¦åœ¨å›¾ä¸­æµåŠ¨ï¼Œå®ç°è‡ªåŠ¨æ±‚èŠ‚ç‚¹æ¢¯åº¦ã€‚

æˆ‘ä»¬é€šè¿‡é‡è½½è¿ç®—ç¬¦å®ç°äº†ä¸€ä¸ªæ”¯æŒè‡ªåŠ¨æ±‚å¯¼çš„ Tensor ç±»ï¼Œç”¨ä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’ demo æµ‹è¯•äº†è‡ªåŠ¨æ±‚å¯¼ã€‚å½“ç„¶è¿™åªæ˜¯æœ€åŸºæœ¬çš„èƒ½å®ç°è‡ªåŠ¨æ±‚å¯¼åŠŸèƒ½çš„ demoï¼Œä»å®ç°çš„è§’åº¦ä¸Šçœ‹è¿˜æœ‰å¾ˆå¤šéœ€è¦ä¼˜åŒ–çš„åœ°æ–¹ï¼ˆå†…å­˜å¼€é”€ã€è¿ç®—é€Ÿåº¦ç­‰ï¼‰ï¼Œç¬”è€…æœ‰ç©ºä¼šç»§ç»­æ·±å…¥ç ”ç©¶ï¼Œè¯»è€…å¦‚æœæœ‰å…´è¶£ä¹Ÿå¯ä»¥è‡ªè¡ŒæŸ¥é˜…ç›¸å…³èµ„æ–™ã€‚Peace out. ğŸ¤˜

<br>

### å‚è€ƒèµ„æ–™

- [PyTorch Doc](https://pytorch.org/docs/stable/index.html)
- [PyTorch Autograd Explained - In-depth Tutorial](https://www.youtube.com/watch?v=MswxJw-8PvE)
- [joelgrus/autograd](https://github.com/joelgrus/autograd)
- [Automatic Differentiation in Machine Learning: a Survey](http://jmlr.org/papers/volume18/17-468/17-468.pdf)

<br><br>
